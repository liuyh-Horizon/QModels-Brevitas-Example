{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19945f-c1a1-4165-bbf0-72cc103323d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065dea11-c38a-45b8-92c3-aaea080b6aa3",
   "metadata": {},
   "source": [
    "# Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6068b-e104-4a0f-b762-32c64ed650c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 28*28\n",
    "hidden_0 = 256\n",
    "hidden_1 = 256\n",
    "hidden_2 = 256\n",
    "output_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8621b8-7b24-4f8a-a062-2825f89148fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc0   = nn.Linear(input_length, hidden_0)\n",
    "        self.bn0   = nn.BatchNorm1d(hidden_0)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        self.drop0 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc1   = nn.Linear(hidden_0, hidden_1)\n",
    "        self.bn1   = nn.BatchNorm1d(hidden_1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2   = nn.Linear(hidden_1, hidden_2)\n",
    "        self.bn2   = nn.BatchNorm1d(hidden_2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.out   = nn.Linear(hidden_2, output_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x.reshape(x.shape[0], -1)\n",
    "        out = self.drop0(self.relu0(self.bn0(self.fc0(out))))\n",
    "        out = self.drop1(self.relu1(self.bn1(self.fc1(out))))\n",
    "        out = self.drop2(self.relu2(self.bn2(self.fc2(out))))\n",
    "        out = self.out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8049789-95d8-4be0-827b-d645f6b832fc",
   "metadata": {},
   "source": [
    "# Random Seed Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e7bea-af24-4b98-ad7b-46b9dc513273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2140cbe6-3ff6-4449-8ad0-9b5fd55c843a",
   "metadata": {},
   "source": [
    "# Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eed741-50c3-485d-80ee-9fda10a29d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size):\n",
    "    transform = transforms.ToTensor()\n",
    "    dataset = datasets.MNIST(\"data\", train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(\"data\", train=False, download=True, transform=transform)\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "    return (\n",
    "        DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_set, batch_size=batch_size),\n",
    "        DataLoader(test_dataset, batch_size=batch_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f16b78-004f-4ec7-8dff-08269b3668e8",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c59a40-2ecf-45cb-a642-349567f859f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test(config, save_dir):\n",
    "    set_random_seed()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    log_file = os.path.join(save_dir, \"training_log.txt\")\n",
    "    resume_path = os.path.join(save_dir, \"checkpoint.pth\")\n",
    "\n",
    "    epochs, batch_size, lr1, lr2, lr3, dropout = config\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(batch_size)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MLP(dropout=dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "\n",
    "    start_epoch = 0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    if os.path.exists(resume_path):\n",
    "        ckpt = torch.load(resume_path)\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "        history = ckpt[\"history\"]\n",
    "        start_epoch = ckpt[\"epoch\"] + 1\n",
    "        print(f\"üîÑ Resuming from epoch {start_epoch}\")\n",
    "    else:\n",
    "        with open(log_file, \"w\") as f:\n",
    "            f.write(\"Training started\\n\")\n",
    "\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        loss_total, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "                loss_total += loss.item() * x.size(0)\n",
    "                _, pred = out.max(1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += x.size(0)\n",
    "        return loss_total / total, correct / total\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        lr = lr1 if epoch < 0.5 * epochs else lr2 if epoch < 0.8 * epochs else lr3\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            _, pred = out.max(1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_loader)\n",
    "        train_loss /= total\n",
    "        train_acc = correct / total\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        log_line = (f\"Epoch {epoch+1}/{epochs} - LR: {lr:.5f} | \"\n",
    "                    f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "                    f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "        print(log_line)\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log_line + \"\\n\")\n",
    "\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"history\": history\n",
    "        }, resume_path)\n",
    "\n",
    "    # Final Test\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "    final_line = f\"Final Test Loss: {test_loss:.4f} Accuracy: {test_acc:.4f}\"\n",
    "    print(final_line)\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(final_line + \"\\n\")\n",
    "        f.write(\"Training completed\\n\")\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"final_model.pth\"))\n",
    "\n",
    "    # Plot and show\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_plot.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94bf6b-a568-4020-85fc-df780bd16f14",
   "metadata": {},
   "source": [
    "# Exploring Best Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee501e-ad20-4ed1-9db3-31b65619be0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Hyperparameter_searching(root_path, epochs, batch_sizes, lr_pool, dropout_rates):\n",
    "    output_root = root_path\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    \n",
    "    epochs = epochs\n",
    "    batch_sizes = batch_sizes\n",
    "    lr_pool = lr_pool\n",
    "    \n",
    "    # Generate descending LR combinations\n",
    "    lr_combos = [combo for combo in itertools.product(lr_pool, repeat=3)\n",
    "                 if (combo[0] > combo[1] > combo[2]) or (combo[0] == combo[1] == combo[2])]\n",
    "    \n",
    "    all_runs = [\n",
    "        f\"batch{bs}_lr{lr1}_{lr2}_{lr3}\"\n",
    "        for bs, (lr1, lr2, lr3) in itertools.product(batch_sizes, lr_combos)\n",
    "    ]\n",
    "    \n",
    "    completed_runs = []\n",
    "    for run in all_runs:\n",
    "        log_path = os.path.join(output_root, run, \"training_log.txt\")\n",
    "        if os.path.exists(log_path):\n",
    "            with open(log_path, \"r\") as f:\n",
    "                if any(\"Training completed\" in line for line in f):\n",
    "                    completed_runs.append(run)\n",
    "    \n",
    "    if len(completed_runs) == len(all_runs):\n",
    "        print(\"‚úÖ All combinations completed. Restarting from scratch...\")\n",
    "        for run in all_runs:\n",
    "            shutil.rmtree(os.path.join(output_root, run), ignore_errors=True)\n",
    "        if os.path.exists(os.path.join(output_root, \"summary.csv\")):\n",
    "            os.remove(os.path.join(output_root, \"summary.csv\"))\n",
    "        completed_runs = []\n",
    "    \n",
    "    results = []\n",
    "    for batch_size, (lr1, lr2, lr3), dropout in itertools.product(batch_sizes, lr_combos, dropout_rates):\n",
    "        run_name = f\"batch{batch_size}_lr{lr1}_{lr2}_{lr3}_dp{dropout}\"\n",
    "        save_dir = os.path.join(output_root, run_name)\n",
    "    \n",
    "        if run_name in completed_runs:\n",
    "            print(f\"‚úÖ Skipping {run_name}\")\n",
    "            with open(os.path.join(save_dir, \"training_log.txt\")) as f:\n",
    "                for line in f:\n",
    "                    if \"Final Test Loss\" in line:\n",
    "                        acc = float(line.strip().split()[-1])\n",
    "                        results.append((run_name, acc))\n",
    "            continue\n",
    "    \n",
    "        print(f\"üöÄ Running: {run_name}\")\n",
    "        acc = train_validate_test([epochs, batch_size, lr1, lr2, lr3, dropout], save_dir)\n",
    "        results.append((run_name, acc))\n",
    "    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_config, best_acc = results[0]\n",
    "    print(f\"üèÜ Best Config: {best_config} Accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(output_root, \"summary.csv\"), \"w\") as f:\n",
    "        f.write(\"Config,Accuracy\\n\")\n",
    "        for name, acc in results:\n",
    "            f.write(f\"{name},{acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47f939-f775-4e76-b1e2-5c35d33c6475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path = \"MLP_256_float_regular_ReLU\"\n",
    "\n",
    "epochs = 100\n",
    "batch_sizes = [64, 128, 256]\n",
    "dropout_rates = [0.0, 0.2, 0.5]\n",
    "lr_pool = [0.01, 0.005, 0.002, 0.001]\n",
    "\n",
    "Hyperparameter_searching(root_path, epochs, batch_sizes, lr_pool, dropout_rates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
