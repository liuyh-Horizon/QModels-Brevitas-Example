{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6a89c7-5184-4ec6-82c0-a44407ea1628",
   "metadata": {},
   "source": [
    "# Quick Tutorial of Brevitas for Hardware-Oriented QNN Traning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba60ba-01dd-4856-8e23-64a9c8cdae41",
   "metadata": {},
   "source": [
    "##### *Author: Yuhao Liu, Chair of Processor Design, TU Dresden, Email: yuhao.liu1@tu-dresden.de*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1f9a2-df3b-4f56-9ae8-b4468f52ce1e",
   "metadata": {},
   "source": [
    "## Section I: Library Import for Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bfaac-f6b2-4cd3-bde2-8d6070ee01f8",
   "metadata": {},
   "source": [
    "#### 1.1 Improt PyTorch Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0f9143-bc4e-4884-87b6-16fa560bff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8386fb-8b49-4a2f-bc89-61bcc8b58768",
   "metadata": {},
   "source": [
    "#### 1.2 Import Brevitas 0.11 Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74a9215-568d-4904-9b31-0dfccfab48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.nn as qnn # Import QNN layers in Brevitas\n",
    "from brevitas.quant_tensor.int_quant_tensor import IntQuantTensor # Import Integer Quantization types for QNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a18705-e6c2-47dc-90f0-5a7b7cadcbce",
   "metadata": {},
   "source": [
    "## Section II: Quantization Type in Brevitas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bbd81-86d0-471a-bc28-76500e55cbd4",
   "metadata": {},
   "source": [
    "#### 2.1 How to define Integer Quantization Type tensors in Brevitas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8af92-785e-4c34-858b-efdbd1193730",
   "metadata": {},
   "source": [
    "IntQuantType is used for training and inferring the QNN models in Brevitas, which consists of six attitudes: \n",
    "\n",
    "<ol>\n",
    "  <li>value: the non-quantized raw value</li>\n",
    "  <li>scale: the scale rate to quantize the raw value</li>\n",
    "  <li>zero_point: the zero point to quantize the raw value</li>\n",
    "  <li>bit_width: the bitwidth of quantized value</li>\n",
    "  <li>signed: If the quantized value is signed</li>\n",
    "  <li>training: If this value is used in training</li>\n",
    "</ol>\n",
    "\n",
    "Therefore, we can try to create the IntQuantType objects.\n",
    "\n",
    "First, we create three raw value tensors for our IntQuantType object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1350e65b-b2cf-47ef-bd6f-0b497c5f8fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Value for 1-d integer quantized tensor: \n",
      " tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) \n",
      "\n",
      "Raw Value for 2-d integer quantized tensor: \n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]]) \n",
      "\n",
      "Raw Value for 3-d integer quantized tensor: \n",
      " tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15.]],\n",
      "\n",
      "        [[16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27.],\n",
      "         [28., 29., 30., 31.]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "value_1d = torch.arange(0, 10, dtype=torch.float32)\n",
    "value_2d = torch.arange(0, 16, dtype=torch.float32).reshape(4, 4)\n",
    "value_3d = torch.arange(0, 32, dtype=torch.float32).reshape(2, 4, 4)\n",
    "\n",
    "print(\"Raw Value for 1-d integer quantized tensor: \\n\", value_1d, \"\\n\")\n",
    "print(\"Raw Value for 2-d integer quantized tensor: \\n\", value_2d, \"\\n\")\n",
    "print(\"Raw Value for 3-d integer quantized tensor: \\n\", value_3d, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab70995-1f6a-48d3-a3e2-947d588d15d2",
   "metadata": {},
   "source": [
    "Then we can define the scale rate, zero point for three IntQuantType tensors. Their are fixed as 8 bit and unsigned data for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190f7ef9-e422-48a9-948d-1b9a1a654bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_1d, scale_2d, scale_3d = 0.1, 0.2, 0.5\n",
    "zero_point_1d, zero_point_2d, zero_point_3d = 1.0, 2.0, 3.0\n",
    "bit_width = 8\n",
    "signed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba7c701-b069-4795-b159-c0b89435585a",
   "metadata": {},
   "source": [
    "Therefore, we can create three IntQuantType tensor as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b157a1-c7a0-4019-9829-4dece435a396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-d integer quantized tensor: \n",
      " IntQuantTensor(value=tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]), scale=0.10000000149011612, zero_point=1.0, bit_width=8.0, signed_t=False, training_t=False) \n",
      "\n",
      "2-d integer quantized tensor: \n",
      " IntQuantTensor(value=tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]]), scale=0.20000000298023224, zero_point=2.0, bit_width=8.0, signed_t=False, training_t=False) \n",
      "\n",
      "3-d integer quantized tensor: \n",
      " IntQuantTensor(value=tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15.]],\n",
      "\n",
      "        [[16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23.],\n",
      "         [24., 25., 26., 27.],\n",
      "         [28., 29., 30., 31.]]]), scale=0.5, zero_point=3.0, bit_width=8.0, signed_t=False, training_t=False) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "int_tensor_1d = IntQuantTensor(value = value_1d, \n",
    "                               scale=scale_1d, \n",
    "                               zero_point=zero_point_1d, \n",
    "                               bit_width=bit_width, \n",
    "                               signed=signed, \n",
    "                               training=False)\n",
    "int_tensor_2d = IntQuantTensor(value = value_2d, \n",
    "                               scale=scale_2d, \n",
    "                               zero_point=zero_point_2d, \n",
    "                               bit_width=bit_width, \n",
    "                               signed=signed, \n",
    "                               training=False)\n",
    "int_tensor_3d = IntQuantTensor(value = value_3d, \n",
    "                               scale=scale_3d, \n",
    "                               zero_point=zero_point_3d, \n",
    "                               bit_width=bit_width, \n",
    "                               signed=signed, \n",
    "                               training=False)\n",
    "\n",
    "print(\"1-d integer quantized tensor: \\n\", int_tensor_1d, \"\\n\")\n",
    "print(\"2-d integer quantized tensor: \\n\", int_tensor_2d, \"\\n\")\n",
    "print(\"3-d integer quantized tensor: \\n\", int_tensor_3d, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126127b0-6d5e-4cdb-9692-7d881fad3032",
   "metadata": {},
   "source": [
    "Therefore, we can find that the data in IntQuantType is actually saved as floating-point, not the quantized integers. If we want to see the real integer value of these IntQuantType tensors, we can try the following two methods:\n",
    "\n",
    "1. Using the int() function of IntQuantType, which can output the quantized integer tensor based on the value, scale rate, and zero point.\n",
    "This function follows $Q_{out} = \\frac{Raw_{in}}{scale\\_rate} + zero\\_point$\n",
    "2. Manually extract the raw value, scale rate, and zero point to round the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec12e2da-9f36-464f-ad3d-ad00977b5772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real quantized 1-d tensor: \n",
      " tensor([ 1., 11., 21., 31., 41., 51., 61., 71., 81., 91.]) \n",
      "\n",
      "Real quantized 2-d tensor: \n",
      " tensor([[ 2.,  7., 12., 17.],\n",
      "        [22., 27., 32., 37.],\n",
      "        [42., 47., 52., 57.],\n",
      "        [62., 67., 72., 77.]]) \n",
      "\n",
      "Real quantized 3-d tensor: \n",
      " tensor([[[ 3.,  5.,  7.,  9.],\n",
      "         [11., 13., 15., 17.],\n",
      "         [19., 21., 23., 25.],\n",
      "         [27., 29., 31., 33.]],\n",
      "\n",
      "        [[35., 37., 39., 41.],\n",
      "         [43., 45., 47., 49.],\n",
      "         [51., 53., 55., 57.],\n",
      "         [59., 61., 63., 65.]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "real_int_tensor_1d = int_tensor_1d.int(float_datatype=True)\n",
    "real_int_tensor_2d = int_tensor_2d.int(float_datatype=True)\n",
    "real_int_tensor_3d = int_tensor_3d.int(float_datatype=True)\n",
    "\n",
    "print(\"Real quantized 1-d tensor: \\n\", real_int_tensor_1d, \"\\n\")\n",
    "print(\"Real quantized 2-d tensor: \\n\", real_int_tensor_2d, \"\\n\")\n",
    "print(\"Real quantized 3-d tensor: \\n\", real_int_tensor_3d, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07df6d-02f3-4b77-980a-76758bbfbbae",
   "metadata": {},
   "source": [
    "We can notice that the int() function needs a \"float_datatype=True\" to convert the quantized data to a floating-point format. Otherwise, it cannot be used in network training and inference. \n",
    "\n",
    "Another method is manually extracting the raw value, scale rate, and zero point, computing the result following the equation above, and rounding the output as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa784c4c-a8df-4213-9ec8-63449c23e7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually quantized 1-d tensor: \n",
      " tensor([ 1., 11., 21., 31., 41., 51., 61., 71., 81., 91.]) \n",
      "\n",
      "Manually quantized 2-d tensor: \n",
      " tensor([[ 2.,  7., 12., 17.],\n",
      "        [22., 27., 32., 37.],\n",
      "        [42., 47., 52., 57.],\n",
      "        [62., 67., 72., 77.]]) \n",
      "\n",
      "Manually quantized 3-d tensor: \n",
      " tensor([[[ 3.,  5.,  7.,  9.],\n",
      "         [11., 13., 15., 17.],\n",
      "         [19., 21., 23., 25.],\n",
      "         [27., 29., 31., 33.]],\n",
      "\n",
      "        [[35., 37., 39., 41.],\n",
      "         [43., 45., 47., 49.],\n",
      "         [51., 53., 55., 57.],\n",
      "         [59., 61., 63., 65.]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_scale_1d, extract_scale_2d, extract_scale_3d = int_tensor_1d.scale, int_tensor_2d.scale, int_tensor_3d.scale\n",
    "extract_zero_point_1d, extract_zero_point_2d, extract_zero_point_3d = int_tensor_1d.zero_point, int_tensor_2d.zero_point, int_tensor_3d.zero_point\n",
    "\n",
    "manual_int_tensor_1d = torch.round((int_tensor_1d.value / int_tensor_1d.scale) + int_tensor_1d.zero_point)\n",
    "manual_int_tensor_2d = torch.round((int_tensor_2d.value / int_tensor_2d.scale) + int_tensor_2d.zero_point)\n",
    "manual_int_tensor_3d = torch.round((int_tensor_3d.value / int_tensor_3d.scale) + int_tensor_3d.zero_point)\n",
    "\n",
    "print(\"Manually quantized 1-d tensor: \\n\", real_int_tensor_1d, \"\\n\")\n",
    "print(\"Manually quantized 2-d tensor: \\n\", real_int_tensor_2d, \"\\n\")\n",
    "print(\"Manually quantized 3-d tensor: \\n\", real_int_tensor_3d, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda99bce-4f96-43c8-bec0-1801261aadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 10\n",
    "in_batch = 3\n",
    "out_features = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef87b7f-8d81-46c2-8660-0a1c7dff298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_torch = nn.Linear(in_features=in_features, out_features=out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c530e6a4-33a4-48ca-8c97-38596d14cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14., 15., 16., 17., 18., 19.],\n",
      "        [20., 21., 22., 23., 24., 25., 26., 27., 28., 29.]]) \n",
      "\n",
      "Input Shape: \n",
      " torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "input = torch.arange(0, (in_features*in_batch), dtype=torch.float32).reshape(in_batch, in_features)\n",
    "print(\"Input: \\n\", input, \"\\n\")\n",
    "print(\"Input Shape: \\n\", input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9165811-7e82-461a-b9df-55684ef93084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      " tensor([[  3.3895,  -5.7051,  -0.0827,   1.8867],\n",
      "        [  8.4728, -19.1678,  -4.5732,   9.1544],\n",
      "        [ 13.5561, -32.6305,  -9.0636,  16.4221]], grad_fn=<MmBackward0>) \n",
      "\n",
      "Output Shape: \n",
      " torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "output = fc_torch(input)\n",
    "print(\"Output: \\n\", output, \"\\n\")\n",
    "print(\"Output Shape: \\n\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84c03c02-5139-4087-83c0-4f7dfaf7c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bit_width = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa039d42-49b3-40fc-b0c7-75590c751e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_brevtias = qnn.QuantLinear(\n",
    "                         in_features, \n",
    "                         out_features, \n",
    "                         weight_bit_width=weight_bit_width,\n",
    "                         weight_quant_type=QuantType.INT,\n",
    "                         bias=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b21cf2ee-5980-481a-a023-d3025d4bb89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14., 15., 16., 17., 18., 19.],\n",
      "        [20., 21., 22., 23., 24., 25., 26., 27., 28., 29.]]) \n",
      "\n",
      "Input Shape: \n",
      " torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "input = torch.arange(0, (in_features*in_batch), dtype=torch.float32).reshape(in_batch, in_features)\n",
    "print(\"Input: \\n\", input, \"\\n\")\n",
    "print(\"Input Shape: \\n\", input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a86c6cc2-cfbf-4c3c-a4e4-9b64bd137fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      " tensor([[ 1.3777e+00,  6.3361e+00, -1.0303e+00, -2.4129e-03],\n",
      "        [ 5.8656e+00,  1.7797e+01, -3.3949e+00, -2.9195e-01],\n",
      "        [ 1.0353e+01,  2.9258e+01, -5.7594e+00, -5.8149e-01]],\n",
      "       grad_fn=<MmBackward0>) \n",
      "\n",
      "Output Shape: \n",
      " torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "output = fc_brevtias(input)\n",
    "print(\"Output: \\n\", output, \"\\n\")\n",
    "print(\"Output Shape: \\n\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "199d94a0-e362-451b-a0f2-24d306b84550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1873,  0.0713,  0.1197, -0.2085,  0.2534, -0.2766,  0.1688,  0.0643,\n",
       "         -0.0056,  0.0702],\n",
       "        [ 0.1553, -0.1768,  0.1419,  0.2165,  0.2388, -0.1434,  0.1745,  0.2389,\n",
       "          0.0745,  0.2245],\n",
       "        [ 0.1911,  0.1431, -0.1837, -0.0698, -0.2489, -0.1370, -0.3064,  0.1281,\n",
       "          0.2084,  0.0405],\n",
       "        [-0.0802,  0.1470, -0.0577, -0.0128, -0.0954,  0.1145, -0.1578,  0.2424,\n",
       "         -0.2232,  0.0974]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_brevtias.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4f4595b-9100-45e3-bbd8-206b1b738868",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = nn.BatchNorm1d(out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06c14502-f1f2-4891-a454-17badfbc4a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      " tensor([[-1.2247e+00, -1.2247e+00,  1.2247e+00,  1.2246e+00],\n",
      "        [ 9.3746e-08,  1.6303e-08,  1.0984e-07,  6.7152e-07],\n",
      "        [ 1.2247e+00,  1.2247e+00, -1.2247e+00, -1.2246e+00]],\n",
      "       grad_fn=<NativeBatchNormBackward0>) \n",
      "\n",
      "Output Shape: \n",
      " torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "output = bn(output)\n",
    "print(\"Output: \\n\", output, \"\\n\")\n",
    "print(\"Output Shape: \\n\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ab29213-9d81-466f-9f98-3c77e5b43e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_bit_width = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f6a9ffe-33ef-452e-80e6-a61aa5c9ad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      " tensor([[-1.3513e-01, -1.3513e-01,  1.0896e+00,  1.0895e+00],\n",
      "        [ 4.6873e-08,  8.1513e-09,  5.4920e-08,  3.3576e-07],\n",
      "        [ 1.0896e+00,  1.0896e+00, -1.3513e-01, -1.3515e-01]],\n",
      "       grad_fn=<GeluBackward0>) \n",
      "\n",
      "Output Shape: \n",
      " torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "gelu = nn.GELU()\n",
    "output = gelu(output)\n",
    "print(\"Output: \\n\", output, \"\\n\")\n",
    "print(\"Output Shape: \\n\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9829d633-9f80-4d1f-825b-38e8e950a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      " QuantTensor(value=tensor([[-0., -0., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., -0., -0.]], grad_fn=<MulBackward0>), scale=tensor(1.), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True)) \n",
      "\n",
      "Output Shape: \n",
      " torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "quant_gelu = qnn.QuantIdentity(\n",
    "                         quant_type='int',\n",
    "                         scaling_impl_type='const',\n",
    "                         bit_width=quan_bit_width,\n",
    "                         min_val=-128.0,\n",
    "                         max_val=127.0, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "output = quant_gelu(output)\n",
    "print(\"Output: \\n\", output, \"\\n\")\n",
    "print(\"Output Shape: \\n\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc93088-85e2-489b-b626-bc726062a372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      " QuantTensor(value=tensor([[-0.8410, -0.8410,  0.8345,  0.8345],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.8345,  0.8345, -0.8410, -0.8410]], grad_fn=<MulBackward0>), scale=tensor(0.0066, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True)) \n",
      "\n",
      "Output Shape: \n",
      " torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "output = quant_relu(output)\n",
    "print(\"Output: \\n\", output, \"\\n\")\n",
    "print(\"Output Shape: \\n\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d752f8-aa1f-456e-bb63-57950c035165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      " tensor([[-128., -128.,  127.,  127.],\n",
      "        [   0.,    0.,    0.,    0.],\n",
      "        [ 127.,  127., -128., -128.]], grad_fn=<AddBackward0>) \n",
      "\n",
      "Output Shape: \n",
      " torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "output = output._pre_round_int_value\n",
    "print(\"Output: \\n\", output, \"\\n\")\n",
    "print(\"Output Shape: \\n\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab1f25-774a-4c01-9411-3ff56c9b7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size=(3,3)   \n",
    "\n",
    "in_channels1=1\n",
    "out_channels1=64 \n",
    "\n",
    "in_channels2=64\n",
    "out_channels2=64\n",
    "\n",
    "input_size = 7*7*64 \n",
    "\n",
    "weight_bit_width = 1\n",
    "act_bit_width = 1\n",
    "\n",
    "hidden1 = 64   \n",
    "num_classes = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b96d6-4a7f-4a2a-851f-eee5e409b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCV_W8A8(Module):\n",
    "    def __init__(self):\n",
    "        super(TCV_W8A8, self).__init__()\n",
    "        \n",
    "        self.input = qnn.QuantIdentity(\n",
    "                         quant_type='int',\n",
    "                         scaling_impl_type='const',\n",
    "                         bit_width=quan_bit_width,\n",
    "                         min_val=-128.0,\n",
    "                         max_val=127.0, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "        \n",
    "        self.conv1 = qnn.QuantConv2d( \n",
    "                         in_channels=in_channels1,\n",
    "                         out_channels=out_channels1,\n",
    "                         kernel_size=kernel_size, \n",
    "                         stride=1, \n",
    "                         padding=1,\n",
    "                         weight_bit_width=weight_bit_width,\n",
    "                         weight_quant_type=QuantType.BINARY,\n",
    "                         bias=False\n",
    "                     )\n",
    "        \n",
    "        self.bn1   = nn.BatchNorm2d(out_channels1)\n",
    "        self.relu1 = qnn.QuantReLU(\n",
    "                         bit_width=act_bit_width, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "        \n",
    "        self.pool1 = qnn.QuantMaxPool2d(2, return_quant_tensor=True)\n",
    "        \n",
    "        self.conv2 = qnn.QuantConv2d( \n",
    "                         in_channels=in_channels2,\n",
    "                         out_channels=out_channels2,\n",
    "                         kernel_size=kernel_size, \n",
    "                         stride=1, \n",
    "                         padding=1,\n",
    "                         weight_bit_width=weight_bit_width,\n",
    "                         weight_quant_type=QuantType.BINARY,\n",
    "                         bias=False\n",
    "                     )\n",
    "        \n",
    "        self.bn2   = nn.BatchNorm2d(out_channels2)\n",
    "        self.relu2 = qnn.QuantReLU(\n",
    "                         bit_width=act_bit_width, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "        \n",
    "        self.pool2 = qnn.QuantMaxPool2d(2, return_quant_tensor=True)\n",
    "        \n",
    "        self.fc1   = qnn.QuantLinear(\n",
    "                         input_size, \n",
    "                         hidden1, \n",
    "                         weight_bit_width=weight_bit_width,\n",
    "                         weight_quant_type=QuantType.BINARY,\n",
    "                         bias=False\n",
    "                     )\n",
    "        \n",
    "        self.bn3   = nn.BatchNorm1d(hidden1)\n",
    "        self.relu3 = qnn.QuantReLU(\n",
    "                         bit_width=act_bit_width, \n",
    "                         return_quant_tensor=True\n",
    "                     )\n",
    "        \n",
    "        self.out   = qnn.QuantLinear(\n",
    "                         hidden1, \n",
    "                         num_classes, \n",
    "                         weight_bit_width=weight_bit_width,\n",
    "                         weight_quant_type=QuantType.BINARY,\n",
    "                         bias=False\n",
    "                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.input(x) # MNIST INPUT 28x28 channel 1 => 28x28x1\n",
    "        out = self.pool1(self.relu1(self.bn1(self.conv1(out))))\n",
    "                            # Conv OUTPUT 28X28 channel 64 => 28x28x64\n",
    "                            # MaxPool OUTPUT 14x14 channel 64 => 14x14x64\n",
    "        out = self.pool2(self.relu2(self.bn2(self.conv2(out))))\n",
    "                            # Conv OUTPUT 14X14 channel 64 => 14x14x64\n",
    "                            # MaxPool OUTPUT 7x7 channel 64 => 7x7x64\n",
    "        out = out.reshape(out.shape[0], -1) # FC INPUT 7X7X64\n",
    "        out = self.relu3(self.bn3(self.fc1(out))) # FC OUTPUT 64\n",
    "        out = self.out(out) # OUTPUT 10\n",
    "        return out\n",
    "   \n",
    "model = TCV_W1A1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19929d03-4774-4f1d-aa57-f6b90eb9c572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
